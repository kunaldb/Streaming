{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IoT Event Data Generation\n",
    "\n",
    "Generate synthetic IoT-style events with fixed columns and a dynamic JSON column. Use for demos, testing pipelines, or streaming (e.g. Azure Event Hub → Delta/Iceberg).\n",
    "\n",
    "**Quick start:** Set parameters in **Cell 1 (Config)** and run cells in order. Batch output can be JSON/file; streaming uses Event Hub (Databricks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Config (edit here)\n",
    "\n",
    "All tunable parameters in one place. Change these to match your environment and volume size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d16b770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ----- Batch generation -----\n",
    "NUM_EVENTS = 50\n",
    "MIN_KEYS = 5\n",
    "MAX_KEYS = 25\n",
    "RANDOM_SEED = None  # Set to an int (e.g. 42) for reproducible data\n",
    "\n",
    "# ----- Paths (local or Databricks Volume) -----\n",
    "CATALOG_VOLUME_PATH = \"/Volumes/kunal/default/iot_data\"  # e.g. \"/Volumes/<catalog>/<schema>/<volume>\" or \"./output\"\n",
    "OUTPUT_DIR = CATALOG_VOLUME_PATH\n",
    "OUTPUT_FILENAME_PREFIX = \"iot_events\"\n",
    "SAVE_JSON = True\n",
    "\n",
    "# ----- Time window for generated events -----\n",
    "BASE_TIME_DAYS_AGO = 1\n",
    "TIME_SPREAD_HOURS = 24\n",
    "\n",
    "# ----- Event Hub / streaming (secrets from config_local.py, not committed) -----\n",
    "try:\n",
    "    from config_local import EVENTHUB_CONNECTION_STRING\n",
    "except ImportError:\n",
    "    EVENTHUB_CONNECTION_STRING = os.environ.get(\"EVENTHUB_CONNECTION_STRING\", \"\")\n",
    "EVENTHUB_NAMESPACE = \"oneenv-eventhub\"\n",
    "EVENTHUB_NAME = \"one-env-windturbine\"\n",
    "BATCH_SIZE = 10\n",
    "SLEEP_INTERVAL_SEC = 2\n",
    "\n",
    "# ----- Delta / Iceberg (streaming sink) -----\n",
    "DELTA_TABLE_NAME = \"kunal.default.iot_events_stream\"\n",
    "DELTA_TABLE_PATH = \"/mnt/delta/your_delta_table_path\"\n",
    "CHECKPOINT_DIR = f\"{CATALOG_VOLUME_PATH}/checkpoints/delta_stream\"\n",
    "ICEBERG_TABLE_NAME = \"kunal.default.iot_events_stream_iceberg\"\n",
    "CHECKPOINT_DIR_ICEBERG = f\"{CATALOG_VOLUME_PATH}/checkpoints_iceberg/delta_stream\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c466ddab",
   "metadata": {},
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "959b8f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data catalogs (fixed columns, key pool, enums)\n",
    "\n",
    "All schema and value lists in one cell for easy customisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIXED_COLUMNS = [\n",
    "    \"event_id\", \"timestamp\", \"device_type\", \"firmware_version\", \"battery_level\", \"sensor_id\",\n",
    "    \"temperature\", \"humidity\", \"pressure\", \"status\", \"region\", \"zone\", \"model\", \"version\",\n",
    "    \"signal_strength\", \"uptime_hours\", \"fault_count\", \"last_calibration\", \"maintenance_due\",\n",
    "    \"created_at\", \"updated_at\", \"source\", \"env\",\n",
    "]\n",
    "\n",
    "NESTED_KEYS = {\n",
    "    \"location\": lambda: {\"latitude\": round(random.uniform(-90, 90), 4), \"longitude\": round(random.uniform(-180, 180), 4), \"altitude\": round(random.uniform(0, 1000), 1)},\n",
    "    \"diagnostics\": lambda: {\"cpu_usage\": round(random.uniform(0, 100), 1), \"memory_usage\": round(random.uniform(0, 100), 1), \"disk_space_mb\": random.randint(100, 1024)},\n",
    "    \"config\": lambda: {\"poll_interval_sec\": random.randint(1, 60), \"retry_count\": random.randint(0, 5)},\n",
    "}\n",
    "\n",
    "IOT_KEY_POOL = [\n",
    "    \"device_type\", \"firmware_version\", \"battery_level\", \"location\", \"diagnostics\", \"status\",\n",
    "    \"error_codes\", \"last_service\", \"warranty_expiry\", \"config\", \"sensor_id\", \"model\",\n",
    "    \"temperature\", \"humidity\", \"signal_strength\", \"uptime_hours\", \"last_calibration\",\n",
    "    \"fault_count\", \"maintenance_due\", \"version\", \"region\", \"zone\", \"tags\",\n",
    "]\n",
    "\n",
    "STATUS_VALUES = [\"idle\", \"active\", \"standby\", \"error\", \"maintenance\", \"offline\"]\n",
    "DEVICE_TYPES = [\"Type-A\", \"Type-B\", \"Type-C\", \"Type-D\"]\n",
    "REGIONS = [\"north\", \"south\", \"east\", \"west\", \"central\"]\n",
    "SOURCES = [\"gateway\", \"edge\", \"cloud\", \"ingest\"]\n",
    "ENVS = [\"prod\", \"staging\", \"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generator functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dynamic_value(key, nested_keys=None, device_types=None, status_values=None, key_pool=None):\n",
    "    nested_keys = nested_keys or NESTED_KEYS\n",
    "    device_types = device_types or DEVICE_TYPES\n",
    "    status_values = status_values or STATUS_VALUES\n",
    "    if key in nested_keys:\n",
    "        return nested_keys[key]()\n",
    "    if key == \"device_type\":\n",
    "        return random.choice(device_types)\n",
    "    if key == \"firmware_version\":\n",
    "        return f\"v{random.randint(1, 3)}.{random.randint(0, 9)}\"\n",
    "    if key == \"battery_level\":\n",
    "        return round(random.uniform(0, 100), 1)\n",
    "    if key == \"status\":\n",
    "        return random.choice(status_values)\n",
    "    if key in (\"error_codes\", \"last_service\", \"warranty_expiry\", \"last_calibration\", \"maintenance_due\"):\n",
    "        return random.choice([None, None, None, \"2025-01-15\", round(random.uniform(1, 100), 1)])\n",
    "    if key in (\"temperature\", \"humidity\", \"uptime_hours\", \"signal_strength\", \"fault_count\"):\n",
    "        return round(random.uniform(0, 100), 1) if random.random() > 0.2 else None\n",
    "    if key == \"sensor_id\":\n",
    "        return f\"SENSOR-{random.randint(1, 999):03d}\"\n",
    "    if key in (\"model\", \"version\", \"region\", \"zone\"):\n",
    "        return random.choice([f\"M-{random.randint(1, 20)}\", \"v2.0\", \"north\", \"zone-1\", None])\n",
    "    if key == \"tags\":\n",
    "        return random.sample([\"prod\", \"test\", \"edge\", \"critical\"], k=random.randint(0, 3))\n",
    "    return random.choice([random.randint(0, 100), round(random.uniform(0, 50), 2), \"unknown\", None])\n",
    "\n",
    "\n",
    "def generate_fixed_row(event_id, base_time, device_types=None, status_values=None, regions=None, sources=None, envs=None):\n",
    "    device_types = device_types or DEVICE_TYPES\n",
    "    status_values = status_values or STATUS_VALUES\n",
    "    regions = regions or REGIONS\n",
    "    sources = sources or SOURCES\n",
    "    envs = envs or ENVS\n",
    "    t = base_time + timedelta(minutes=random.randint(0, 60 * max(1, TIME_SPREAD_HOURS)))\n",
    "    return {\n",
    "        \"event_id\": event_id,\n",
    "        \"timestamp\": t.isoformat() + \"Z\",\n",
    "        \"device_type\": random.choice(device_types),\n",
    "        \"firmware_version\": f\"v{random.randint(1, 3)}.{random.randint(0, 9)}\",\n",
    "        \"battery_level\": round(random.uniform(0, 100), 1),\n",
    "        \"sensor_id\": f\"SENSOR-{random.randint(1, 999):03d}\",\n",
    "        \"temperature\": round(random.uniform(-10, 50), 1) if random.random() > 0.1 else None,\n",
    "        \"humidity\": round(random.uniform(0, 100), 1) if random.random() > 0.1 else None,\n",
    "        \"pressure\": round(random.uniform(1000, 1025), 1) if random.random() > 0.1 else None,\n",
    "        \"status\": random.choice(status_values),\n",
    "        \"region\": random.choice(regions),\n",
    "        \"zone\": f\"zone-{random.randint(1, 10)}\",\n",
    "        \"model\": f\"M-{random.randint(1, 20)}\",\n",
    "        \"version\": f\"v{random.randint(1, 3)}.{random.randint(0, 9)}\",\n",
    "        \"signal_strength\": random.randint(1, 5) if random.random() > 0.2 else None,\n",
    "        \"uptime_hours\": round(random.uniform(0, 8760), 1) if random.random() > 0.2 else None,\n",
    "        \"fault_count\": random.randint(0, 20) if random.random() > 0.3 else None,\n",
    "        \"last_calibration\": (t - timedelta(days=random.randint(1, 365))).isoformat()[:10] if random.random() > 0.5 else None,\n",
    "        \"maintenance_due\": (t + timedelta(days=random.randint(1, 90))).isoformat()[:10] if random.random() > 0.5 else None,\n",
    "        \"created_at\": (t - timedelta(minutes=random.randint(0, 60))).isoformat() + \"Z\",\n",
    "        \"updated_at\": t.isoformat() + \"Z\",\n",
    "        \"source\": random.choice(sources),\n",
    "        \"env\": random.choice(envs),\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_dynamic_json(min_keys=None, max_keys=None, key_pool=None):\n",
    "    min_k = min_keys if min_keys is not None else MIN_KEYS\n",
    "    max_k = max_keys if max_keys is not None else MAX_KEYS\n",
    "    pool = key_pool or IOT_KEY_POOL\n",
    "    n = random.randint(min_k, min(max_k, len(pool)))\n",
    "    keys = random.sample(pool, n)\n",
    "    return {k: make_dynamic_value(k) for k in keys}\n",
    "\n",
    "\n",
    "def generate_iot_row(event_id, base_time, **kwargs):\n",
    "    row = generate_fixed_row(event_id, base_time, **kwargs)\n",
    "    row[\"json_column\"] = generate_dynamic_json()\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate batch and (optional) save\n",
    "\n",
    "Single place: build table, DataFrame, column order, and optional JSON export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RANDOM_SEED is not None:\n",
    "    random.seed(RANDOM_SEED)\n",
    "\n",
    "base_time = datetime.now().astimezone().replace(microsecond=0) - timedelta(days=BASE_TIME_DAYS_AGO)\n",
    "table_rows = [generate_iot_row(i + 1, base_time) for i in range(NUM_EVENTS)]\n",
    "\n",
    "df = pd.DataFrame(table_rows)\n",
    "cols = [c for c in FIXED_COLUMNS if c in df.columns] + [\"json_column\"]\n",
    "df = df[[c for c in cols if c in df.columns]]\n",
    "\n",
    "if SAVE_JSON:\n",
    "    out_path = Path(OUTPUT_DIR)\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_file = out_path / f\"{OUTPUT_FILENAME_PREFIX}_{timestamp}.json\"\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(table_rows, f, indent=2, default=str)\n",
    "    print(f\"Saved {len(table_rows)} events to {output_file}\")\n",
    "\n",
    "print(f\"Table: {len(df)} rows × {len(df.columns)} columns (23 fixed + 1 JSON column)\")\n",
    "print(\"Columns:\", list(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preview = df.copy()\n",
    "df_preview[\"json_column\"] = df_preview[\"json_column\"].apply(\n",
    "    lambda x: (json.dumps(x)[:80] + \"...\") if isinstance(x, dict) else (str(x)[:80] + \"...\")\n",
    ")\n",
    "df_preview.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Streaming (Databricks + Event Hub)\n",
    "\n",
    "The cells below require:\n",
    "- PySpark and a Spark session (`spark`)\n",
    "- Azure Event Hub connection string (set in Config or via secret)\n",
    "- Databricks Volume for checkpoints (or equivalent paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Event Hub connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_string = EVENTHUB_CONNECTION_STRING or os.environ.get(\"EVENTHUB_CONNECTION_STRING\", \"\")\n",
    "if not connection_string:\n",
    "    print(\"Warning: EVENTHUB_CONNECTION_STRING not set. Set it in Config or env before streaming.\")\n",
    "else:\n",
    "    print(f\"Event Hub: {EVENTHUB_NAMESPACE}.servicebus.windows.net\")\n",
    "    print(f\"Topic: {EVENTHUB_NAME}\")\n",
    "    print(f\"Rate: ~{BATCH_SIZE / SLEEP_INTERVAL_SEC:.1f} events/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Stream IoT data to Event Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "rate_stream = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", max(1, int(BATCH_SIZE / SLEEP_INTERVAL_SEC) or 1)).load()\n",
    "event_counter = [0]\n",
    "stream_base_time = datetime.now().astimezone().replace(microsecond=0)\n",
    "\n",
    "def generate_event_udf(trigger_value):\n",
    "    event_counter[0] += 1\n",
    "    return json.dumps(generate_iot_row(event_counter[0], stream_base_time))\n",
    "\n",
    "generate_event = udf(generate_event_udf, StringType())\n",
    "\n",
    "def extract_sensor_id_udf(json_str):\n",
    "    try:\n",
    "        return json.loads(json_str).get(\"sensor_id\", \"unknown\")\n",
    "    except Exception:\n",
    "        return \"unknown\"\n",
    "\n",
    "extract_sensor_id = udf(extract_sensor_id_udf, StringType())\n",
    "iot_stream = rate_stream.withColumn(\"value\", generate_event(col(\"value\"))).withColumn(\"key\", extract_sensor_id(col(\"value\"))).select(\"key\", \"value\")\n",
    "\n",
    "jaas_config = f'kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"{connection_string}\";'\n",
    "\n",
    "if connection_string:\n",
    "    query = iot_stream.writeStream.format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", f\"{EVENTHUB_NAMESPACE}.servicebus.windows.net:9093\") \\\n",
    "        .option(\"kafka.sasl.mechanism\", \"PLAIN\") \\\n",
    "        .option(\"kafka.security.protocol\", \"SASL_SSL\") \\\n",
    "        .option(\"kafka.sasl.jaas.config\", jaas_config) \\\n",
    "        .option(\"topic\", EVENTHUB_NAME) \\\n",
    "        .option(\"checkpointLocation\", f\"{CATALOG_VOLUME_PATH}/checkpoints/eventhub_stream\") \\\n",
    "        .outputMode(\"append\").start()\n",
    "    print(\"Stream started. Press Stop to stop.\")\n",
    "    query.awaitTermination()\n",
    "else:\n",
    "    print(\"Skipped: set EVENTHUB_CONNECTION_STRING to run streaming.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Read stream from Event Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "\n",
    "jaas_config = f'kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"{connection_string}\";'\n",
    "\n",
    "kafka_stream = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", f\"{EVENTHUB_NAMESPACE}.servicebus.windows.net:9093\") \\\n",
    "    .option(\"kafka.sasl.mechanism\", \"PLAIN\") \\\n",
    "    .option(\"kafka.security.protocol\", \"SASL_SSL\") \\\n",
    "    .option(\"kafka.sasl.jaas.config\", jaas_config) \\\n",
    "    .option(\"subscribe\", EVENTHUB_NAME) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "kafka_stream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Parse JSON and convert json_column to VARIANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import parse_json\n",
    "\n",
    "iot_schema = StructType([\n",
    "    StructField(\"event_id\", IntegerType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"device_type\", StringType(), True),\n",
    "    StructField(\"firmware_version\", StringType(), True),\n",
    "    StructField(\"battery_level\", DoubleType(), True),\n",
    "    StructField(\"sensor_id\", StringType(), True),\n",
    "    StructField(\"temperature\", DoubleType(), True),\n",
    "    StructField(\"humidity\", DoubleType(), True),\n",
    "    StructField(\"pressure\", DoubleType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"region\", StringType(), True),\n",
    "    StructField(\"zone\", StringType(), True),\n",
    "    StructField(\"model\", StringType(), True),\n",
    "    StructField(\"version\", StringType(), True),\n",
    "    StructField(\"signal_strength\", IntegerType(), True),\n",
    "    StructField(\"uptime_hours\", DoubleType(), True),\n",
    "    StructField(\"fault_count\", IntegerType(), True),\n",
    "    StructField(\"last_calibration\", StringType(), True),\n",
    "    StructField(\"maintenance_due\", StringType(), True),\n",
    "    StructField(\"created_at\", StringType(), True),\n",
    "    StructField(\"updated_at\", StringType(), True),\n",
    "    StructField(\"source\", StringType(), True),\n",
    "    StructField(\"env\", StringType(), True),\n",
    "    StructField(\"json_column\", StringType(), True),\n",
    "])\n",
    "\n",
    "parsed_stream = kafka_stream.select(\n",
    "    col(\"key\").cast(\"string\").alias(\"kafka_key\"),\n",
    "    col(\"timestamp\").alias(\"kafka_timestamp\"),\n",
    "    from_json(col(\"value\").cast(\"string\"), iot_schema).alias(\"data\")\n",
    ").select(\"kafka_key\", \"kafka_timestamp\", \"data.*\")\n",
    "\n",
    "parsed_stream_with_variant = parsed_stream.withColumn(\n",
    "    \"json_column_variant\", parse_json(col(\"json_column\"))\n",
    ").drop(\"json_column\")\n",
    "\n",
    "parsed_stream_with_variant.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Write stream to Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_delta = parsed_stream_with_variant.writeStream.format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_DIR) \\\n",
    "    .toTable(DELTA_TABLE_NAME)\n",
    "\n",
    "print(f\"Writing to Delta: {DELTA_TABLE_NAME}\")\n",
    "query_delta.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec492f5",
   "metadata": {},
   "source": [
    "### 12. Stop streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d9059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to stop active streaming queries (Event Hub and/or Delta)\n",
    "stopped = []\n",
    "for name in (\"query\", \"query_delta\"):\n",
    "    try:\n",
    "        q = globals().get(name)\n",
    "        if q is not None and getattr(q, \"isActive\", lambda: False)():\n",
    "            q.stop()\n",
    "            stopped.append(name)\n",
    "            print(f\"Stopped {name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{name}: {e}\")\n",
    "if not stopped:\n",
    "    print(\"No active streams to stop (or run the stream cells first).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
